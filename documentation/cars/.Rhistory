fill = "steelblue",
color = "white",
alpha = 0.8
) +
geom_density(color = "red", linewidth = 1) +
labs(
title = "Distribution of Model Residuals",
x = "Residuals (Actual - Predicted Price)",
y = "Density"
) +
theme_minimal()
# Residuals vs. Fitted Plot
p1 <- ggplot(model_data, aes(x = Fitted, y = Residuals)) +
geom_point(alpha = 0.6, color = "steelblue") +
geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
geom_smooth(method = "loess", color = "darkgreen", se = FALSE) +
labs(
title = "Residuals vs. Fitted Values",
subtitle = "Check for homoscedasticity (constant variance)",
x = "Fitted Values (Predicted Price)",
y = "Residuals (Actual - Predicted)"
) +
theme_minimal()
p1
# QQ-Plot
p2 <- ggplot(model_data, aes(sample = Residuals)) +
stat_qq(color = "steelblue") +
stat_qq_line(color = "red", linetype = "dashed") +
labs(
title = "Normal Q-Q Plot",
subtitle = "Check for normality of residuals",
x = "Theoretical Quantiles",
y = "Sample Quantiles"
) +
theme_minimal()
p2
train_data$Kilometres_num2 <- train_data$Kilometres_num^2
train_data$age2 <- train_data$age^2
train_data$Cylinders2 <- train_data$Cylinders^2
EDA_selected = c('Model_encoded', 'Location_encoded', 'Car_Suv_encoded', 'Kilometres_num2', 'Cylinders2', 'ColourExtInt_encoded', 'FuelType_encoded','DriveType_encoded','age2')
model_formula_poly <- reformulate(EDA_selected, response = "log_Price")
lm_poly_model <- train(
model_formula_poly,
data = train_data,
method = "lm",
trControl = trainControl(method = "cv", number = 5)
)
# View the model
summary(lm_poly_model$finalModel)
# Predict on test data
test_data$Kilometres_num2 <- test_data$Kilometres_num^2
test_data$age2 <- test_data$age^2
test_data$Cylinders2 <- test_data$Cylinders^2
test_data$log_Price_pred <- predict(lm_poly_model, newdata = test_data)
# To revert to the unscaled values, we calculate the exponent of the logged values
test_data$Price_pred <- exp(test_data$log_Price_pred)
# --- 1. Plot: Predicted vs. Actual (log scale) ---
ggplot(test_data, aes(x = log(Price), y = log_Price_pred)) +
geom_point(alpha = 0.4, color = "blue") +
geom_abline(slope = 1, color = "red", linetype = "dashed") +
labs(title = "Predicted vs. Actual Prices (Log Scale)",
x = "Actual log(Price)", y = "Predicted log(Price)") +
theme_minimal()
# --- 2. Plot: Predicted vs. Actual (original price scale) ---
ggplot(test_data, aes(x = Price, y = Price_pred)) +
geom_point(alpha = 0.4, color = "darkgreen") +
geom_abline(slope = 1, color = "red", linetype = "dashed") +
labs(title = "Predicted vs. Actual Prices (Original Scale)",
x = "Actual Price", y = "Predicted Price") +
scale_x_continuous(labels = scales::dollar) +
scale_y_continuous(labels = scales::dollar) +
theme_minimal()
# residuals <- residuals(lm_model$finalModel)
# Extract residuals and fitted values
model_data <- data.frame(
Fitted = fitted(lm_model$finalModel),
Residuals = residuals(lm_model$finalModel)
)
# Converting to dataframe for ggplot
# residuals_df <- data.frame(Residuals = residuals)
# Creating histogram with density overlay with density curve
ggplot(model_data, aes(x = Residuals)) +
geom_histogram(
aes(y = after_stat(density)),
bins = 50,
fill = "steelblue",
color = "white",
alpha = 0.8
) +
geom_density(color = "red", linewidth = 1) +
labs(
title = "Distribution of Model Residuals",
x = "Residuals (Actual - Predicted Price)",
y = "Density"
) +
theme_minimal()
# Residuals vs. Fitted Plot
p1 <- ggplot(model_data, aes(x = Fitted, y = Residuals)) +
geom_point(alpha = 0.6, color = "steelblue") +
geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
geom_smooth(method = "loess", color = "darkgreen", se = FALSE) +
labs(
title = "Residuals vs. Fitted Values",
subtitle = "Check for homoscedasticity (constant variance)",
x = "Fitted Values (Predicted Price)",
y = "Residuals (Actual - Predicted)"
) +
theme_minimal()
p1
# QQ-Plot
p2 <- ggplot(model_data, aes(sample = Residuals)) +
stat_qq(color = "steelblue") +
stat_qq_line(color = "red", linetype = "dashed") +
labs(
title = "Normal Q-Q Plot",
subtitle = "Check for normality of residuals",
x = "Theoretical Quantiles",
y = "Sample Quantiles"
) +
theme_minimal()
p2
# A code chunk labeled setup.
# this chunk will not appear in the final document (no code, no output shown), but it will run
# Sets global options for all subsequent code chunks.
#echo = TRUE means R code will be displayed in the output (HTML/PDF).
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(conflicted)
library(corrplot)
library(purrr)
library(caret)
library(nnet)
library(pROC)
library(car)
conflict_prefer("filter", "dplyr")
conflict_prefer("lag", "dplyr")
conflict_prefer("select", "dplyr")
paste("Loading dataset from '", getwd(), "'")
df <- read_csv("../../data/preprocessed/cars_v2.csv", show_col_types = FALSE)
summary(df)
TARGET = 'Price'
# Create a new feature combining Brand + Model (e.g., "Toyota_Corolla")
EDA_selected = c('Model_encoded', 'Location_encoded', 'Car_Suv_encoded', 'Kilometres_num', 'Cylinders', 'ColourExtInt_encoded', 'FuelType_encoded','DriveType_encoded', 'UsedOrNew_encoded', 'age')
selected_data <- df %>% select(all_of(EDA_selected), all_of(TARGET))
# selected_data <- selected_data %>% filter(complete.cases(.))
summary(selected_data)
# Fixes the random number generator to ensure the same data split every time.
set.seed(123)  # For reproducibility
# Splits the data into 80% training and 20% testing, preserving the distribution of Price
train_index <- createDataPartition(selected_data$Price, p = 0.8, list = FALSE)
train_data <- selected_data[train_index, ]
test_data  <- selected_data[-train_index, ]
# Option A: Log-transform Price
# Applies the natural logarithm to Price in the training set only (to avoid data leakage).
# train_data$log_Price <- log(train_data$Price)
# Option B: Remove extreme values (e.g., top/bottom 1%)
# price_cutoffs <- quantile(train_data$log_Price, c(0.01, 0.99))
# train_clean <- train_data %>%
#   filter(log_Price > price_cutoffs[1] & log_Price < price_cutoffs[2])
# Train model
# Use formula syntax (Price ~ . uses all other columns as predictors)
# Create formula dynamically
# log_Price ~ Brand_Encoded + Model_encoded + ...
model_formula <- reformulate(EDA_selected, response = "Price_log")
# Train model
lm_model <- train(
model_formula,
data = train_data,
method = "lm",
trControl = trainControl(method = "cv", number = 5)
)
# A code chunk labeled setup.
# this chunk will not appear in the final document (no code, no output shown), but it will run
# Sets global options for all subsequent code chunks.
#echo = TRUE means R code will be displayed in the output (HTML/PDF).
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(conflicted)
library(corrplot)
library(purrr)
library(caret)
library(nnet)
library(pROC)
library(car)
conflict_prefer("filter", "dplyr")
conflict_prefer("lag", "dplyr")
conflict_prefer("select", "dplyr")
paste("Loading dataset from '", getwd(), "'")
df <- read_csv("../../data/preprocessed/cars_v2.csv", show_col_types = FALSE)
summary(df)
TARGET <- c('Price', 'Price_log')
# Create a new feature combining Brand + Model (e.g., "Toyota_Corolla")
EDA_selected <- c('Model_encoded', 'Location_encoded', 'Car_Suv_encoded', 'Kilometres_num', 'Cylinders', 'ColourExtInt_encoded', 'FuelType_encoded','DriveType_encoded', 'UsedOrNew_encoded', 'age')
selected_data <- df %>% select(all_of(EDA_selected), all_of(TARGET))
# selected_data <- selected_data %>% filter(complete.cases(.))
summary(selected_data)
# Fixes the random number generator to ensure the same data split every time.
set.seed(123)  # For reproducibility
# Splits the data into 80% training and 20% testing, preserving the distribution of Price
train_index <- createDataPartition(selected_data$Price, p = 0.8, list = FALSE)
train_data <- selected_data[train_index, ]
test_data  <- selected_data[-train_index, ]
# Option A: Log-transform Price
# Applies the natural logarithm to Price in the training set only (to avoid data leakage).
# train_data$log_Price <- log(train_data$Price)
# Option B: Remove extreme values (e.g., top/bottom 1%)
# price_cutoffs <- quantile(train_data$log_Price, c(0.01, 0.99))
# train_clean <- train_data %>%
#   filter(log_Price > price_cutoffs[1] & log_Price < price_cutoffs[2])
# Train model
# Use formula syntax (Price ~ . uses all other columns as predictors)
# Create formula dynamically
# log_Price ~ Brand_Encoded + Model_encoded + ...
model_formula <- reformulate(EDA_selected, response = "Price_log")
# Train model
lm_model <- train(
model_formula,
data = train_data,
method = "lm",
trControl = trainControl(method = "cv", number = 5)
)
# Print model coefficients
summary(lm_model$finalModel)
vif(lm_model$finalModel)
# Predict on test data
test_data$log_Price_pred <- predict(lm_model, newdata = test_data)
# To revert to the unscaled values, we calculate the exponent of the logged values
test_data$Price_pred <- exp(test_data$log_Price_pred)
# --- 1. Plot: Predicted vs. Actual (log scale) ---
ggplot(test_data, aes(x = log(Price), y = log_Price_pred)) +
geom_point(alpha = 0.4, color = "blue") +
geom_abline(slope = 1, color = "red", linetype = "dashed") +
labs(title = "Predicted vs. Actual Prices (Log Scale)",
x = "Actual log(Price)", y = "Predicted log(Price)") +
theme_minimal()
# --- 2. Plot: Predicted vs. Actual (original price scale) ---
ggplot(test_data, aes(x = Price, y = Price_pred)) +
geom_point(alpha = 0.4, color = "darkgreen") +
geom_abline(slope = 1, color = "red", linetype = "dashed") +
labs(title = "Predicted vs. Actual Prices (Original Scale)",
x = "Actual Price", y = "Predicted Price") +
scale_x_continuous(labels = scales::dollar) +
scale_y_continuous(labels = scales::dollar) +
theme_minimal()
# residuals <- residuals(lm_model$finalModel)
# Extract residuals and fitted values
model_data <- data.frame(
Fitted = fitted(lm_model$finalModel),
Residuals = residuals(lm_model$finalModel)
)
# Converting to dataframe for ggplot
# residuals_df <- data.frame(Residuals = residuals)
# Creating histogram with density overlay with density curve
ggplot(model_data, aes(x = Residuals)) +
geom_histogram(
aes(y = after_stat(density)),
bins = 50,
fill = "steelblue",
color = "white",
alpha = 0.8
) +
geom_density(color = "red", linewidth = 1) +
labs(
title = "Distribution of Model Residuals",
x = "Residuals (Actual - Predicted Price)",
y = "Density"
) +
theme_minimal()
# Residuals vs. Fitted Plot
p1 <- ggplot(model_data, aes(x = Fitted, y = Residuals)) +
geom_point(alpha = 0.6, color = "steelblue") +
geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
geom_smooth(method = "loess", color = "darkgreen", se = FALSE) +
labs(
title = "Residuals vs. Fitted Values",
subtitle = "Check for homoscedasticity (constant variance)",
x = "Fitted Values (Predicted Price)",
y = "Residuals (Actual - Predicted)"
) +
theme_minimal()
p1
# QQ-Plot
p2 <- ggplot(model_data, aes(sample = Residuals)) +
stat_qq(color = "steelblue") +
stat_qq_line(color = "red", linetype = "dashed") +
labs(
title = "Normal Q-Q Plot",
subtitle = "Check for normality of residuals",
x = "Theoretical Quantiles",
y = "Sample Quantiles"
) +
theme_minimal()
p2
train_data$Kilometres_num2 <- train_data$Kilometres_num^2
train_data$age2 <- train_data$age^2
train_data$Cylinders2 <- train_data$Cylinders^2
EDA_selected = c('Model_encoded', 'Location_encoded', 'Car_Suv_encoded', 'Kilometres_num2', 'Cylinders2', 'ColourExtInt_encoded', 'FuelType_encoded','DriveType_encoded','age2')
model_formula_poly <- reformulate(EDA_selected, response = "Price_log")
lm_poly_model <- train(
model_formula_poly,
data = train_data,
method = "lm",
trControl = trainControl(method = "cv", number = 5)
)
# View the model
summary(lm_poly_model$finalModel)
# Predict on test data
test_data$Kilometres_num2 <- test_data$Kilometres_num^2
test_data$age2 <- test_data$age^2
test_data$Cylinders2 <- test_data$Cylinders^2
test_data$log_Price_pred <- predict(lm_poly_model, newdata = test_data)
# To revert to the unscaled values, we calculate the exponent of the logged values
test_data$Price_pred <- exp(test_data$log_Price_pred)
# --- 1. Plot: Predicted vs. Actual (log scale) ---
ggplot(test_data, aes(x = log(Price), y = log_Price_pred)) +
geom_point(alpha = 0.4, color = "blue") +
geom_abline(slope = 1, color = "red", linetype = "dashed") +
labs(title = "Predicted vs. Actual Prices (Log Scale)",
x = "Actual log(Price)", y = "Predicted log(Price)") +
theme_minimal()
# --- 2. Plot: Predicted vs. Actual (original price scale) ---
ggplot(test_data, aes(x = Price, y = Price_pred)) +
geom_point(alpha = 0.4, color = "darkgreen") +
geom_abline(slope = 1, color = "red", linetype = "dashed") +
labs(title = "Predicted vs. Actual Prices (Original Scale)",
x = "Actual Price", y = "Predicted Price") +
scale_x_continuous(labels = scales::dollar) +
scale_y_continuous(labels = scales::dollar) +
theme_minimal()
# residuals <- residuals(lm_model$finalModel)
# Extract residuals and fitted values
model_data <- data.frame(
Fitted = fitted(lm_model$finalModel),
Residuals = residuals(lm_model$finalModel)
)
# Converting to dataframe for ggplot
# residuals_df <- data.frame(Residuals = residuals)
# Creating histogram with density overlay with density curve
ggplot(model_data, aes(x = Residuals)) +
geom_histogram(
aes(y = after_stat(density)),
bins = 50,
fill = "steelblue",
color = "white",
alpha = 0.8
) +
geom_density(color = "red", linewidth = 1) +
labs(
title = "Distribution of Model Residuals",
x = "Residuals (Actual - Predicted Price)",
y = "Density"
) +
theme_minimal()
# Residuals vs. Fitted Plot
p1 <- ggplot(model_data, aes(x = Fitted, y = Residuals)) +
geom_point(alpha = 0.6, color = "steelblue") +
geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
geom_smooth(method = "loess", color = "darkgreen", se = FALSE) +
labs(
title = "Residuals vs. Fitted Values",
subtitle = "Check for homoscedasticity (constant variance)",
x = "Fitted Values (Predicted Price)",
y = "Residuals (Actual - Predicted)"
) +
theme_minimal()
p1
# QQ-Plot
p2 <- ggplot(model_data, aes(sample = Residuals)) +
stat_qq(color = "steelblue") +
stat_qq_line(color = "red", linetype = "dashed") +
labs(
title = "Normal Q-Q Plot",
subtitle = "Check for normality of residuals",
x = "Theoretical Quantiles",
y = "Sample Quantiles"
) +
theme_minimal()
p2
df <- df %>%
extract(
Engine,                 # The source column containing engine info
into = c("Cylinders", "Engine_L"),  # New columns to create
regex = "(\\d+) cyl, (\\d+\\.?\\d*) L",  # Pattern to match
convert = TRUE          # Automatically convert to appropriate data types
)
# A code chunk labeled setup.
# this chunk will not appear in the final document (no code, no output shown), but it will run
# Sets global options for all subsequent code chunks.
#echo = TRUE means R code will be displayed in the output (HTML/PDF).
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(dplyr)
library(ggplot2)
library(conflicted)
conflict_prefer("filter", "dplyr")
conflict_prefer("lag", "dplyr")
conflict_prefer("select", "dplyr")
paste("Loading dataset from '", getwd(), "'")
df <- read_csv("../../data/raw/australian_vehicle_prices.csv", show_col_types = FALSE)
colnames(df)[colnames(df) == "Car/Suv"] <- "Car_Suv"
PREDICTORS <- c('Brand', 'Year', 'Model',
'Car_Suv', 'UsedOrNew',
'Transmission', 'Kilometres', 'Location', 'Engine',     'ColourExtInt', 'FuelType','DriveType','BodyType')
TARGET <- 'Price'
df <- df[, c(PREDICTORS, TARGET)]
df
df <- df %>%
extract(
Engine,                 # The source column containing engine info
into = c("Cylinders", "Engine_L"),  # New columns to create
regex = "(\\d+) cyl, (\\d+\\.?\\d*) L",  # Pattern to match
convert = TRUE          # Automatically convert to appropriate data types
)
df
df[df == "-" | df == "-/-"] <- NA
rows_before <- nrow(df)
print(paste("Rows before removing NA:", rows_before))
# Step 2: Remove rows with any NA values
df <- na.omit(df)
# Step 3: Count rows after removing NA
rows_after <- nrow(df)
print(paste("Rows after removing NA:", rows_after))
# target.encode <- function(df, target, col) {
#   # Convert column name to symbol for tidy evaluation
#   col_sym <- sym(col)
#   encoded_col_name <- paste0(col, "_encoded")
#
#   # Step 1: Calculate mean target per group
#   avg_target <- df %>%
#     group_by(!!col_sym) %>%
#     summarise(mean_target = mean(.data[[target]], na.rm = TRUE), .groups = "drop") %>%
#     arrange(mean_target)
#
#   # Step 2: Assign ranks
#   avg_target <- avg_target %>%
#     mutate(!!encoded_col_name := as.numeric(factor(mean_target, levels = unique(mean_target))) - 1)
#
#   # Step 3: Join encoded column back to original dataframe
#   df <- df %>%
#     left_join(avg_target %>% select(!!col_sym, !!sym(encoded_col_name)), by = col)
#
#   return(df)
# }
target.encode <- function(train_df, test_df, target, col) {
col_sym <- sym(col)
encoded_col_name <- paste0(col, "_encoded")
# Step 1: Calculate mean target per group (train set only)
avg_target <- train_df %>%
group_by(!!col_sym) %>%
summarise(mean_target = mean(.data[[target]], na.rm = TRUE), .groups = "drop") %>%
arrange(mean_target)
# Step 2: Assign ranks
avg_target <- avg_target %>%
mutate(!!encoded_col_name := as.numeric(factor(mean_target, levels = unique(mean_target))) - 1)
# Step 3: Join encoded column to both train and test
train_df <- train_df %>%
left_join(avg_target %>% select(!!col_sym, !!sym(encoded_col_name)), by = col)
test_df <- test_df %>%
left_join(avg_target %>% select(!!col_sym, !!sym(encoded_col_name)), by = col)
# Optionally fill NAs in test set (unseen categories) with a default, e.g. median or -1
test_df[[encoded_col_name]][is.na(test_df[[encoded_col_name]])] <- -1
return(list(train = train_df, test = test_df))
}
COLS_TO_ENCODE <- c('Brand', 'Model', 'Car_Suv', 'UsedOrNew', 'Transmission', 'DriveType', 'FuelType', 'ColourExtInt', 'Location', 'BodyType')
set.seed(123)  # For reproducibility
train_index <- createDataPartition(df$Price, p = 0.8, list = FALSE)
train_data <- df[train_index, ]
test_data  <- df[-train_index, ]
#
# df <- reduce(COLS_TO_ENCODE, ~ target.encode(df = .x, target = TARGET, col = .y), .init = df)
#
# df
for (col in COLS_TO_ENCODE) {
encoded <- target.encode(train_data, test_data, target = "Price", col = col)
train_data <- encoded$train
test_data  <- encoded$test
}
df <- rbind(train_data, test_data)
df
df <- df %>%
mutate(Kilometres_num = as.numeric(gsub("[^0-9]", "", Kilometres)))
numeric_cols <- df[, sapply(df, is.numeric)]
current_year <- as.numeric(format(Sys.Date(), "%Y"))
df$age <- current_year - df$Year
df <- df %>% select(-c('Brand', 'Model', 'Car_Suv', 'UsedOrNew', 'Transmission','Year','Kilometres','Location','Engine_L','ColourExtInt','FuelType','DriveType','BodyType'))
num_cols <- df[, sapply(df, is.numeric)]
# Loop through each numeric column
for (colname in names(num_cols)) {
boxplot(num_cols[[colname]],
main = paste("Boxplot of", colname),
ylab = colname)
}
df <- df %>% mutate(Price_log = log10(Price + 1))
detect_outliers_iqr <- function(df, remove = FALSE) {
num_cols <- df[, sapply(df, is.numeric), drop = FALSE]
outlier_flags <- matrix(FALSE, nrow = nrow(df), ncol = ncol(num_cols))
for (i in seq_along(num_cols)) {
x <- num_cols[[i]]
Q1 <- quantile(x, 0.05, na.rm = TRUE)
Q3 <- quantile(x, 0.95, na.rm = TRUE)
IQR_val <- IQR(x, na.rm = TRUE)
lower <- Q1 - 1.5 * IQR_val
upper <- Q3 + 1.5 * IQR_val
outlier_flags[, i] <- x < lower | x > upper
}
if (remove) {
df <- df[!apply(outlier_flags, 1, any), ]
}
return(df)
}
df<- detect_outliers_iqr(df, remove = TRUE)
write.csv(df, file = "../../data/preprocessed/cars_v2.csv", row.names = FALSE)  # Avoid row numbers
