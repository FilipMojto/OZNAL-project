---
title: "EDA for OZNAL project"
author: "Filip Remšík, Filip Mojto"
date: "`r Sys.Date()`" # the date updates automatically when rendered
output:
  pdf_document:
    toc: true
  html_document:
    toc: true # Adds a table of contents (TOC)
    toc_float: true # Makes the TOC float on the side
---

```{r setup, include=FALSE}
# A code chunk labeled setup.
# this chunk will not appear in the final document (no code, no output shown), but it will run

# Sets global options for all subsequent code chunks.
#echo = TRUE means R code will be displayed in the output (HTML/PDF).
knitr::opts_chunk$set(echo = TRUE)

```

## Libraries

```{r}

library(tidyverse)
library(conflicted)
library(corrplot)
library(purrr)
library(caret)
library(nnet)
library(pROC)

conflict_prefer("filter", "dplyr")
conflict_prefer("lag", "dplyr")
conflict_prefer("select", "dplyr")

```

## Dataset

```{r}

paste("Loading dataset from '", getwd(), "'")

df <- read_csv("../../data/preprocessed/cars_v1.csv", show_col_types = FALSE)
summary(df)
```

## Target & Predictors

```{r}

TARGET = 'Price'
# Create a new feature combining Brand + Model (e.g., "Toyota_Corolla")


EDA_selected = c('Brand_Model_encoded', 'Location_encoded', 'Car_Suv_encoded', 'Kilometres_num', 'Cylinders', 'ColourExtInt_encoded', 'FuelType_encoded','DriveType_encoded')
  
selected_data <- df %>% select(all_of(EDA_selected), all_of(TARGET))
selected_data <- selected_data %>% filter(complete.cases(.))
summary(selected_data)

```

## Linear Regression

EDA showed roughly linear relationships between predictors and log target. Thus, we are going to try to fit Linear Regression on this problem.

### Outliers

```{r}

# Function to winsorize
winsorize <- function(x, limits = c(0.01, 0.99)) {
  x[x < quantile(x, limits[1], na.rm = TRUE)] <- quantile(x, limits[1], na.rm = TRUE)
  x[x > quantile(x, limits[2], na.rm = TRUE)] <- quantile(x, limits[2], na.rm = TRUE)
  x
}

# Apply to new features
selected_data <- selected_data %>%
  mutate(across(
    all_of(EDA_selected),  # Use all_of() for safety
    winsorize
  ))
```

### Train/Test Split

Now we can final train the model. First we are going to follow the best practices for machine learning workflow by splitting the data into training and testing dataset. By doing so, we are going to preserve the data integrity. This is because we only apply cross validation on the training set to avoid data leakage and the testing set remains unseen.

We are going to apply 80:20 split. Generally, the fewer observations are present in data the greater the portion of the testing subset should be.

```{r}

# Fixes the random number generator to ensure the same data split every time.
set.seed(123)  # For reproducibility

# Splits the data into 80% training and 20% testing, preserving the distribution of Price

train_index <- createDataPartition(selected_data$Price, p = 0.8, list = FALSE)
train_data <- selected_data[train_index, ]
test_data  <- selected_data[-train_index, ]


# Option A: Log-transform Price
# Applies the natural logarithm to Price in the training set only (to avoid data leakage).
train_data$log_Price <- log(train_data$Price)

# Option B: Remove extreme values (e.g., top/bottom 1%)
# price_cutoffs <- quantile(train_data$log_Price, c(0.01, 0.99))
# train_clean <- train_data %>% 
#   filter(log_Price > price_cutoffs[1] & log_Price < price_cutoffs[2])
```

```{r}
# install.packages("car")
# Check encoding direction
# cor(train_data$Brand_encoded, train_data$Price)  # Should be positive
car::vif(lm_model$finalModel)
```

### Function Fitting & Cross Validation

Our dataset (consisting of 16000 of car listings) is **large enough** to split into 5 folds while still maintaining sufficient training examples in each fold. It's also **computationally efficient** compared to higher *k* or repeated CV. **LOOCV** technique works for very small datasets (\<100 rows), where maximizing training data is crucial. **Stratified k-Fold Cross-Validation** is best suited for classification problems with imbalanced classes.

We're training a **linear model**, which is not prone to over-fitting. Hence, 5-fold CV provides a **good trade-off** between training on sufficient data and testing on unseen data without being overly expensive.

```{r}

# Train model
# Use formula syntax (Price ~ . uses all other columns as predictors)
# Create formula dynamically
# log_Price ~ Brand_Encoded + Model_encoded + ...
model_formula <- reformulate(EDA_selected, response = "log_Price")

# Train model
lm_model <- train(
  model_formula,
  data = train_data,
  method = "lm",
  trControl = trainControl(method = "cv", number = 5)
)

# Print model coefficients
summary(lm_model$finalModel)
```

R-square error represents % of variance in log_Price explained by our model (0.79 = 79%)

The model had Residual standard error of 0.2944 which means 68% of our predictions fall within ±0.295 log-units of the actual price. C**onverting to Original Scale**:

**`exp(0.295) ≈ 1.34`**

**`exp(-0.295) ≈ 0.75`**

This means +34% overprediction (1.34× the true price) and -25% underprediction (0.75× the true price).

Adjusted R-squared is very close to the original error which is implies that all predictors are important and overfitting is unlikely.

The **F-statistic is very high (5453)**, meaning our model explains a lot more variance than the null model. p-value is low which means this test is statistically significant.

### Predicted vs. Actual - Log Scale

Now let's visualize the predicted vs. actual values. This time we are going to make the model predict the testing subset. We also want to compare the values that were scaled using log10 and those which were not.

```{r}

# Predict on test data
test_data$log_Price_pred <- predict(lm_model, newdata = test_data)

# To revert to the unscaled values, we calculate the exponent of the logged values
test_data$Price_pred <- exp(test_data$log_Price_pred)

# --- 1. Plot: Predicted vs. Actual (log scale) ---
ggplot(test_data, aes(x = log(Price), y = log_Price_pred)) +
  geom_point(alpha = 0.4, color = "blue") +
  geom_abline(slope = 1, color = "red", linetype = "dashed") +
  labs(title = "Predicted vs. Actual Prices (Log Scale)",
       x = "Actual log(Price)", y = "Predicted log(Price)") +
  theme_minimal()

# --- 2. Plot: Predicted vs. Actual (original price scale) ---
ggplot(test_data, aes(x = Price, y = Price_pred)) +
  geom_point(alpha = 0.4, color = "darkgreen") +
  geom_abline(slope = 1, color = "red", linetype = "dashed") +
  labs(title = "Predicted vs. Actual Prices (Original Scale)",
       x = "Actual Price", y = "Predicted Price") +
  scale_x_continuous(labels = scales::dollar) +
  scale_y_continuous(labels = scales::dollar) +
  theme_minimal()

```

### Residuals

Interpreting the **residuals histogram** helps us assess whether our linear model's assumptions are reasonably met. If the histogram is symmetric and centered around 0, that's a good sign — it means our model doesn’t systematically over- or under-predict.

```{r}
# residuals <- residuals(lm_model$finalModel)
# Extract residuals and fitted values
model_data <- data.frame(
  Fitted = fitted(lm_model$finalModel),
  Residuals = residuals(lm_model$finalModel)
)

# Converting to dataframe for ggplot
# residuals_df <- data.frame(Residuals = residuals)

# Creating histogram with density overlay with density curve
ggplot(model_data, aes(x = Residuals)) +
  geom_histogram(
    aes(y = after_stat(density)), 
    bins = 50,
    fill = "steelblue",
    color = "white",
    alpha = 0.8
  ) +
  geom_density(color = "red", linewidth = 1) + 
  labs(
    title = "Distribution of Model Residuals",
    x = "Residuals (Actual - Predicted Price)",
    y = "Density"
  ) +
  theme_minimal()


```

From the distribution above we can say that our model doesn't systematically overpredict or underpredict (skewness). Also the spread is not high which implies low prediction error. The distribution is normal which supports model's inference.

#### Homoscedasticity

Linear and polynomial regression require homoscedasticity of predictors. This means they are scattered around 0 without any clear patterns. We are going to check that out by using Scatterplot and QQPlot.

```{r}

# Residuals vs. Fitted Plot
p1 <- ggplot(model_data, aes(x = Fitted, y = Residuals)) +
  geom_point(alpha = 0.6, color = "steelblue") +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
  geom_smooth(method = "loess", color = "darkgreen", se = FALSE) +
  labs(
    title = "Residuals vs. Fitted Values",
    subtitle = "Check for homoscedasticity (constant variance)",
    x = "Fitted Values (Predicted Price)",
    y = "Residuals (Actual - Predicted)"
  ) +
  theme_minimal()

p1

```

#### QQPlot

```{r}

# QQ-Plot
p2 <- ggplot(model_data, aes(sample = Residuals)) +
  stat_qq(color = "steelblue") +
  stat_qq_line(color = "red", linetype = "dashed") +
  labs(
    title = "Normal Q-Q Plot",
    subtitle = "Check for normality of residuals",
    x = "Theoretical Quantiles",
    y = "Sample Quantiles"
  ) +
  theme_minimal()

p2
```

From the plots we can tell that residuals are scattered quite normally around zero. Even though loess can catch non-linear relationships, the smooth line shows almost perfect linear match with the horizontal line. From the QQPlot we can see that they almost perfectly follow the line, though there are deviated values (tails).

## Polynomial Regression

Based on EDA we decided to add Model and Brand to power of 3.

```{r}
train_data$Brand_Model_encoded3 <- train_data$Brand_Model_encoded^3
train_data$Kilometres_num2 <- train_data$Kilometres_num^2
```

```{r}

EDA_selected = c('Brand_Model_encoded3', 'Location_encoded', 'Car_Suv_encoded', 'Kilometres_num2', 'Cylinders', 'ColourExtInt_encoded', 'FuelType_encoded','DriveType_encoded')


model_formula_poly <- reformulate(EDA_selected, response = "log_Price")

lm_poly_model <- train(
  model_formula_poly,
  data = train_data,
  method = "lm",
  trControl = trainControl(method = "cv", number = 5)
)

# View the model
summary(lm_poly_model$finalModel)

```

R-square error represents % of variance in log_Price explained by our model (0.72 = 72%)

### Predicted vs. Actual - Log Scale

```{r}
# Predict on test data
test_data$Brand_Model_encoded3 <- test_data$Brand_Model_encoded^3
test_data$Kilometres_num2 <- test_data$Kilometres_num^2
test_data$log_Price_pred <- predict(lm_poly_model, newdata = test_data)

# To revert to the unscaled values, we calculate the exponent of the logged values
test_data$Price_pred <- exp(test_data$log_Price_pred)

# --- 1. Plot: Predicted vs. Actual (log scale) ---
ggplot(test_data, aes(x = log(Price), y = log_Price_pred)) +
  geom_point(alpha = 0.4, color = "blue") +
  geom_abline(slope = 1, color = "red", linetype = "dashed") +
  labs(title = "Predicted vs. Actual Prices (Log Scale)",
       x = "Actual log(Price)", y = "Predicted log(Price)") +
  theme_minimal()

# --- 2. Plot: Predicted vs. Actual (original price scale) ---
ggplot(test_data, aes(x = Price, y = Price_pred)) +
  geom_point(alpha = 0.4, color = "darkgreen") +
  geom_abline(slope = 1, color = "red", linetype = "dashed") +
  labs(title = "Predicted vs. Actual Prices (Original Scale)",
       x = "Actual Price", y = "Predicted Price") +
  scale_x_continuous(labels = scales::dollar) +
  scale_y_continuous(labels = scales::dollar) +
  theme_minimal()

```

### Residuals

```{r}
# residuals <- residuals(lm_model$finalModel)
# Extract residuals and fitted values
model_data <- data.frame(
  Fitted = fitted(lm_model$finalModel),
  Residuals = residuals(lm_model$finalModel)
)

# Converting to dataframe for ggplot
# residuals_df <- data.frame(Residuals = residuals)

# Creating histogram with density overlay with density curve
ggplot(model_data, aes(x = Residuals)) +
  geom_histogram(
    aes(y = after_stat(density)), 
    bins = 50,
    fill = "steelblue",
    color = "white",
    alpha = 0.8
  ) +
  geom_density(color = "red", linewidth = 1) + 
  labs(
    title = "Distribution of Model Residuals",
    x = "Residuals (Actual - Predicted Price)",
    y = "Density"
  ) +
  theme_minimal()

```

### Homoscedacity

```{r}
# Residuals vs. Fitted Plot
p1 <- ggplot(model_data, aes(x = Fitted, y = Residuals)) +
  geom_point(alpha = 0.6, color = "steelblue") +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
  geom_smooth(method = "loess", color = "darkgreen", se = FALSE) +
  labs(
    title = "Residuals vs. Fitted Values",
    subtitle = "Check for homoscedasticity (constant variance)",
    x = "Fitted Values (Predicted Price)",
    y = "Residuals (Actual - Predicted)"
  ) +
  theme_minimal()

p1
```

### QQ Plot

```{r}
# QQ-Plot
p2 <- ggplot(model_data, aes(sample = Residuals)) +
  stat_qq(color = "steelblue") +
  stat_qq_line(color = "red", linetype = "dashed") +
  labs(
    title = "Normal Q-Q Plot",
    subtitle = "Check for normality of residuals",
    x = "Theoretical Quantiles",
    y = "Sample Quantiles"
  ) +
  theme_minimal()

p2
```

## Conclusion
