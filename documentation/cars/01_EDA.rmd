---
title: "EDA for OZNAL project"
author: "Filip Remšík, Filip Mojto"
date: "`r Sys.Date()`" # the date updates automatically when rendered
output:
  html_document:
    toc: true # Adds a table of contents (TOC)
    toc_float: true # Makes the TOC float on the side
  pdf_document:
    toc: true
---

# 01 - EDA

In this document we perform Exploratory Data Analysis. The input is the hypothesis, target and potential predictors. The output is a list of predictors that best fit for model training, plus recommended models and actions for data preprocessing.

Our hypothesis is as follows:

*"Vehicle prices can be reliably predicted using a combination of manufacturer (brand), vehicle specifications (age, mileage, engine type), and geographic (location) features, with brand and age being the most significant predictors."*

## Setup

```{r setup, include=FALSE}
# A code chunk labeled setup.
# this chunk will not appear in the final document (no code, no output shown), but it will run

# Sets global options for all subsequent code chunks.
#echo = TRUE means R code will be displayed in the output (HTML/PDF).
knitr::opts_chunk$set(echo = TRUE)

```

## Libraries

This is a list of libraries we are going to need throughout this document. Must be run before any other block.

```{r}

library(tidyverse)
library(conflicted)
library(corrplot)
library(purrr)
library(caret)
library(nnet)
library(pROC)
library(scales)  # for comma()

conflict_prefer("filter", "dplyr")
conflict_prefer("lag", "dplyr")
conflict_prefer("select", "dplyr")

```

## Dataset

Now we can load the vehicle dataset. It contains over 15 000 observations and also various car-related attributes like Brand, Model, Year, etc. The dataset should be stored in /data/raw as csv. Having loaded the dataset correctly we can check its basic summary.

```{r}

paste("Loading dataset from '", getwd(), "'")

df <- read_csv("../../data/raw/australian_vehicle_prices.csv", show_col_types = FALSE)

summary(df)
```

From the summary above we can see a lot of predictors are not numerical. A large portion of such columns are categorical. For instance:

1)  *Brand*

2)  *Model*

3)  *Car/Suv*

...

These parameters will need to be converted into numbers using some kind of encoding as we think they can have high influence on the price.

Some attributes like *Title* are redundant as they bring no new information and would only complicate our model. Thus, these attributes won't be included.

## Target & Predictors

After viewing the summary in previous section we have chosen the following predictors:

```{r}

colnames(df)[colnames(df) == "Car/Suv"] <- "Car_Suv"

PREDICTORS <- c('Brand_encoded', 'age', 'Model_encoded', 
                'Car_Suv_encoded', 'UsedOrNew_encoded', 
                'Transmission_encoded', 'Kilometres_num',
                'Cylinders', 'Engine_L', 'DriveType_encoded',
                'FuelType_encoded', 'FuelConsumption_num',
                'ColourExtInt_encoded', 'Location_encoded',
                'BodyType_encoded', 'Doors_num', 'Seats_num')
TARGET <- 'Price'
df

```

## Preprocessing

We already identified some anomalies in the data that need to be dealt with before diving into EDA. First it is the missing values.

### Missing Values

First we calculate the portion of missing values for each attribute.

```{r}

# For a single column (e.g., 'Seats')
colMeans(is.na(df))      


```

From the values we can tell that the portion ranges from 0% to 10%. We can declare that this dataset doesn't contain significant amount of missing values except for attribute *Seats*. We will recommend appropriate actions later in this document but now, for the purpose of EDA, we simply remove all rows that contain any missing values.

```{r}

df <- df %>%
  filter(complete.cases(.))
```

### Encoding

Because data contain a lot of categorical attributes that are relevant to our goal, we've created the following function. This function performs so-called **Target Encoding** which is different from standard one-hot encoding in that that is encodes the categories based on how strong impact the predictor has on the target. Higher average price for the particular predictors means higher assigned number.

```{r}

target.encode <- function(df, target, col) {
  # Convert column name to symbol for tidy evaluation
  col_sym <- sym(col)
  encoded_col_name <- paste0(col, "_encoded")
  
  # Step 1: Calculate mean target per group
  avg_target <- df %>%
    group_by(!!col_sym) %>%
    summarise(mean_target = mean(.data[[target]], na.rm = TRUE), .groups = "drop") %>%
    arrange(mean_target)

  # Step 2: Assign ranks
  avg_target <- avg_target %>%
    mutate(!!encoded_col_name := as.numeric(factor(mean_target, levels = unique(mean_target))) - 1)

  # Step 3: Join encoded column back to original dataframe
  df <- df %>%
    left_join(avg_target %>% select(!!col_sym, !!sym(encoded_col_name)), by = col)

  return(df)
}
```

### Target Encoding

These are columns that need to be encoded. We simply apply the function defined above on each of them.

```{r}

COLS_TO_ENCODE <- c('Brand', 'Model', 'Car_Suv', 'UsedOrNew', 'Transmission', 'DriveType', 'FuelType', 'ColourExtInt', 'Location', 'BodyType')


df <- reduce(COLS_TO_ENCODE, ~ target.encode(df = .x, target = TARGET, col = .y), .init = df)

df

```

### Year -\> Age

Age is time-based feature and thus it introduces inflation. Is is therefore wise to convert it into age using the following formula:

*age = current - manufacturing_year*

```{r}

current_year <- as.numeric(format(Sys.Date(), "%Y"))
df$age <- current_year - df$Year
```

### Engine -\> Cylinders, Engine

```{r}

# extract actually replaces the original column with new ones based on regex match
df <- df %>%
  extract(
    Engine,                 # The source column containing engine info
    into = c("Cylinders", "Engine_L"),  # New columns to create
    regex = "(\\d+) cyl, (\\d+\\.?\\d*) L",  # Pattern to match
    convert = TRUE          # Automatically convert to appropriate data types
  )

df
```

### FuelComsumption

```{r}

df <- df %>%
  mutate(
    FuelConsumption_num = as.numeric(str_extract(FuelConsumption, "\\d+\\.?\\d*"))
  )

df

```

### Doors and Seats

```{r}

df$Doors_num <- as.numeric(gsub("[^0-9]", "", df$Doors))
df$Seats_num <- as.numeric(gsub("[^0-9]", "", df$Seats))
```

After preprocessing is over, we can visualize the final dataset.

```{r}

df
unique(df$UsedOrNew)
summary(df)
```

## EDA

### Linearity

In this section we check how well do the predictors correlate with the target. We will utilize Scatterplots for that. We also try to draw a linear line in the plot to see if there is linear relationship.

```{r}

df <- df %>%
  mutate(Kilometres_num = as.numeric(gsub("[^0-9]", "", Kilometres)))

df <- df %>%
  filter(!is.na(Kilometres_num), !is.na(Price))


show.correlation.plot <- function(df, target, col){
  
  p <- ggplot(df, aes(x = .data[[col]], y = .data[[target]])) +
    geom_point(alpha = 0.5, color = "steelblue") +
    geom_smooth(method = "lm", color = "red") +  # Linear trend line
    labs(title = paste0(target, " vs. ", col), x = col, y = target) +
    theme_minimal()
  
  return(p)
}

plots <- map(PREDICTORS, ~show.correlation.plot(df = df, target = TARGET, col = .x))
walk(plots, print)

```

From the plots above we can say that after applying the encoding the dots are pretty much focused around the line, although there are some outliers. This could imply linear relationship.

### Colinearity

We also need to check whether there are not high correlations between individual predictors. This kind of predictors is duplicate and bring no new information to the model function.

In order to show the multicollinearity in a nice way, we are going to use and display a Correlation Matrix. If the values are above a significant level (0.7 in our case) there is a serious collinearity between predictors.

```{r}

show.corrplot <- function(matrix){
  p <- corrplot(
    matrix,
    method = 'color',
    type = 'lower',
    tl.cex = 0.6,        # much smaller axis text
    cl.cex = 0.6,        # smaller colorbar text
    addCoef.col = "black",
    number.cex = 0.5,    # smaller correlation coefficients
    tl.srt = 45
  )
  
  print(p)
}


cor_data <- df %>% select(all_of(PREDICTORS), all_of(TARGET))
cor_matrix <- cor(cor_data, use = "complete.obs")
show.corrplot(cor_matrix)

```

From the matrix we can see, that several pairs like:

a)  *Kilometres_num* and *age* (0.75)

b)  *Model_encoded* and *Brand_encoded* *(0.61)*

c)  *UsedOrNew_encoded* and *Car_Suv_encoded* *(0.54)*

have quite high correlation.

### Outliers

We also need to visualize the proportion of outliers present in our data. To achieve this in a neat and representative way, we are going to utilize Boxplots.

```{r}


show.boxplot <- function(data, cols, target, bins = 5) {
  data <- data %>%
    mutate(target_binned = cut(.data[[target]], breaks = bins, include.lowest = TRUE))

  plots <- map(cols, ~ggplot(
    data,
    aes(x = target_binned, y = .data[[.x]])) +
      geom_boxplot(aes(group = target_binned)) +
      labs(x = paste0(target, " (binned)"), y = .x) +
      theme_minimal() +
      scale_x_discrete(labels = function(x) gsub(",", "\n", x))  # Wrapping long bin labels
  )

  walk(plots, print)
}

show.boxplot(data = df, cols = PREDICTORS, target = TARGET, bins = 10)
```

From the boxplots we have detected quite a notable portion of outliers. Certain actions will need to be taken.

### Target Distribution

We also need to check the distribution of the target variable to see how balanced the dataset is. We have found out that the prices are highly skewed. By applying the log scale we can deal with these extremes values. Thus, we plot two histograms comparing the original distribution against the log-scaled distribution.

```{r}

# Histogram with log scale
ggplot(df, aes(x = Price)) +
  geom_histogram(fill = "skyblue", color = "black", bins = 30) +
  labs(
    title = "Distribution of Vehicle Prices (Log Scale)",
    x = "Price (Log10 Scale)",
    y = "Count"
  ) +
  theme_minimal()

# Histogram with log scale
ggplot(df, aes(x = Price)) +
  geom_histogram(fill = "skyblue", color = "black", bins = 30) +
  scale_x_log10(labels = scales::comma) +  # Log scale for skewed data
  labs(
    title = "Distribution of Vehicle Prices (Log Scale)",
    x = "Price (Log10 Scale)",
    y = "Count"
  ) +
  theme_minimal()


```

From the plots above we can tell that scaling helped making the distribution more normal.

## Saving to csv

Since we performed some preprocessing we save the data into

*/data/preprocessed/*

```{r}
selected_df <- df %>% select(all_of(PREDICTORS), all_of(TARGET))

write.csv(selected_df, file = "../../data/preprocessed/cars_v1.csv", row.names = FALSE)  # Avoid row numbers

```

## Conclusion

During performing EDA we have found out that:

-   Data needed a lot of preprocessing.

-   We identified a lot of helpful predictors.

-   We identified pretty linear relationships between target and predictors.

-   We detected some portion of missing values -\> Removing is required:

    a) NA

    b) -/- in *ColourExtInt*

    c) - in *Cylinders*, *CylindersinEngine*

-   For target, we have detected quite skewed distribution -\> Log Scale.

-   We also detected some portion of outliers -\> Log scale, IQR, etc.

-   For this problem we recommend training **Linear Regression**, **Polynomial Regression** or **Random Forests**.

-   We recommend trying the following parameters:

    a) **Brand_encoded**

    b) **Model_encoded**

    c) **Location_encoded**

    d) **Car_suv_encoded**

    e) **Kilometers or age**

    f) **Cylinders**

    g) **ColourExtInt**

    h) **FuelType_encoded**

    i) **DriveType_encoded**

    Rest of the parameters have either no information gain (like Title or CylindersinEngine) or have high correlation with other parameters (BodyType, age, etc.) or simply have no great effect on the target (*Doors*, *Seats*, etc.)
